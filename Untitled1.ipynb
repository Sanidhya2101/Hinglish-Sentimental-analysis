{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c62dd898",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Computer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Computer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import csv\n",
    "import re\n",
    "import torch\n",
    "import emoji\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4808e15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(raw_dataset):\n",
    "    dataset=[]\n",
    "    tmp1=[]\n",
    "    tmp2=[]\n",
    "\n",
    "\n",
    "    for i in range(len(raw_dataset)):\n",
    "        if raw_dataset[0][i]==\"meta\":\n",
    "            tmp3=\" \".join(tmp1)\n",
    "            dataset.append(tmp3)\n",
    "            tmp1=[]\n",
    "            tmp3=[]\n",
    "        elif raw_dataset[0][i]==\"@\":\n",
    "            continue\n",
    "        elif raw_dataset[0][i-1]==\"@\":\n",
    "            continue\n",
    "        else:\n",
    "            tmp1.append(str(raw_dataset[0][i]))\n",
    "    dataset.append(\" \".join(tmp1))\n",
    "    dataset.pop(0)\n",
    "\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        if re.search(\"http[.]*\",dataset[i])!=None:\n",
    "            dataset[i]=dataset[i][:re.search(\"http[.]*\",dataset[i]).span()[0]]\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_label(raw_dataset):\n",
    "    y=[]\n",
    "    for i in range(len(raw_dataset)):\n",
    "        if raw_dataset[2][i]==\"positive\":\n",
    "            y.append(0)\n",
    "        elif raw_dataset[2][i]==\"negative\":\n",
    "            y.append(1)\n",
    "        elif raw_dataset[2][i]==\"neutral\":\n",
    "            y.append(2)\n",
    "            \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecce1dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Computer\\AppData\\Local\\Temp\\ipykernel_16108\\1693415903.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  train_data = pd.read_csv('train.txt',sep=\"\\s\",header=None)\n",
      "C:\\Users\\Computer\\AppData\\Local\\Temp\\ipykernel_16108\\1693415903.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  test_data = pd.read_csv('Validation.txt',sep=\"\\s\",header=None)\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('train.txt',sep=\"\\s\",header=None)\n",
    "test_data = pd.read_csv('Validation.txt',sep=\"\\s\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f8fc412",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = get_data(train_data)\n",
    "test_X = get_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9baf8b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = get_label(train_data)\n",
    "test_y = get_label(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1222fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emoji_free_text(text):\n",
    "    return emoji.replace_emoji(text,replace='')\n",
    "\n",
    "def clean_text(text):\n",
    "    punctuation = string.punctuation\n",
    "    text = re.sub(r'(@[A-Za-z0-9_]+)', '', text.lower())\n",
    "    text = re.sub(r'\\&\\w*;', '', text.lower())\n",
    "    text = re.sub(r'\\$\\w*', '', text.lower())\n",
    "    text = re.sub(r'https?:\\/\\/.*\\/\\w*', '', text.lower())\n",
    "    text = re.sub(r'#\\w*', '', text.lower())\n",
    "    text = re.sub(r'^RT[\\s]+', '', text.lower())\n",
    "    text = ''.join(c for c in text.lower() if c <= '\\uFFFF')\n",
    "    text = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", text.lower())\n",
    "    text = re.sub(r'[' + punctuation.replace('@', '') + ']+', ' ', text.lower())\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words = [word.lower() for word in words if word.isalpha()]\n",
    "    return words\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return [word for word in text if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa79a0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_X)):\n",
    "    train_X[i] = clean_text(train_X[i])\n",
    "    train_X[i] = emoji_free_text(train_X[i])\n",
    "    train_X[i] = tokenize(train_X[i])\n",
    "    train_X[i] = remove_stopwords(train_X[i])\n",
    "\n",
    "for i in range(len(test_X)):\n",
    "    test_X[i] = clean_text(test_X[i])\n",
    "    test_X[i] = emoji_free_text(test_X[i])\n",
    "    test_X[i] = tokenize(test_X[i])\n",
    "    test_X[i] = remove_stopwords(test_X[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fe261be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_muse_vecs(muse_file):\n",
    "    with open(muse_file,'r',errors='ignore',encoding=\"utf-8\") as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word_list = line[0: len(line)-300]\n",
    "            curr_word = \"\"\n",
    "            \n",
    "            for t in curr_word_list:\n",
    "                curr_word = curr_word + str(t)+\" \"\n",
    "            curr_word = curr_word.strip()\n",
    "            words.add(curr_word)\n",
    "            try:\n",
    "                word_to_vec_map[curr_word] = np.array(line[-300:],dtype=np.float64)\n",
    "            except:\n",
    "                print(line,len(line))\n",
    "                \n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        words.add(\"nokey\")\n",
    "        word_to_vec_map[\"nokey\"] = np.zeros((300,), dtype=np.float64)\n",
    "\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fb6cf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, index_to_words, word_to_vec_map = read_muse_vecs('MUSE/data/wiki.en.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da977991",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input,Bidirectional,Dense,Dropout,LSTM,Activation\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8cb2d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_(maxLen):\n",
    "    \n",
    "    input_layer = Input(shape = (maxLen,300))\n",
    "    \n",
    "    x = Bidirectional(LSTM(128,activation='tanh',return_sequences=True))(input_layer)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Bidirectional(LSTM(128,activation='tanh',return_sequences=True))(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Bidirectional(LSTM(128,activation='tanh',return_sequences=True))(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Bidirectional(LSTM(128,activation='tanh',return_sequences=True))(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    x = Dense(1)(x)\n",
    "    x = Activation(\"softmax\")(x)\n",
    "    \n",
    "    return Model(inputs=input_layer, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99d80307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_maxlength_doc(docs):\n",
    "    maxLen = max([len(doc) for doc in docs])\n",
    "    return maxLen\n",
    "\n",
    "def docs_to_vector(docs, vec_map, maxLen):\n",
    "    vectors = []\n",
    "    for doc in docs:\n",
    "        vector = []\n",
    "        for token in doc:\n",
    "            if token.lower() in vec_map:\n",
    "                vector.append(vec_map[token.lower()])\n",
    "            else:\n",
    "                vector.append(vec_map[\"nokey\"])\n",
    "        #padd sequence for max length\n",
    "        pad = maxLen - len(vector)\n",
    "        if pad > 0:\n",
    "            padv = np.zeros((300,),dtype=np.float64)\n",
    "            for i in range(pad):\n",
    "                vector.append(padv)\n",
    "          \n",
    "        #adjust the vector if it greater than max length\n",
    "        if pad < 0:\n",
    "            vector = vector[:pad]\n",
    "        vectors.append(vector)\n",
    "        \n",
    "    return np.array(vectors)\n",
    "\n",
    "\n",
    "maxLen = get_maxlength_doc(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93be3782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxLen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0ddc063",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_(maxLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "388ea380",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edd9ed61",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = docs_to_vector(train_X,word_to_vec_map,maxLen)\n",
    "Y_train = np.array(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d938ad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.00582921, -0.0706627 , -0.0406877 , ..., -0.0235659 ,\n",
       "         -0.0435458 ,  0.00886712],\n",
       "        [-0.0914173 , -0.0237519 , -0.0455965 , ..., -0.0544716 ,\n",
       "         -0.0244974 , -0.0500255 ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ]],\n",
       "\n",
       "       [[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [-0.00911191, -0.0752531 ,  0.00084857, ..., -0.00855384,\n",
       "          0.0640495 ,  0.0729606 ],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ]],\n",
       "\n",
       "       [[-0.00949767,  0.0115615 , -0.0770523 , ..., -0.0178562 ,\n",
       "         -0.00170476,  0.00726752],\n",
       "        [ 0.0996716 ,  0.0654681 , -0.0562365 , ...,  0.0287334 ,\n",
       "         -0.0786734 ,  0.0112698 ],\n",
       "        [-0.107033  ,  0.0652514 , -0.111801  , ...,  0.0435615 ,\n",
       "         -0.0688869 , -0.0248593 ],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.0241551 , -0.00937273, -0.089395  , ..., -0.00798493,\n",
       "         -0.0449953 ,  0.0608698 ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [-0.0519614 ,  0.0435911 , -0.00194893, ..., -0.0463938 ,\n",
       "         -0.0359229 , -0.0345871 ],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ]],\n",
       "\n",
       "       [[ 0.0441524 ,  0.0478513 , -0.0517962 , ...,  0.0292616 ,\n",
       "         -0.0942267 , -0.0120807 ],\n",
       "        [-0.012782  , -0.0276779 , -0.034071  , ..., -0.0146588 ,\n",
       "          0.014266  , -0.0187865 ],\n",
       "        [ 0.0241551 , -0.00937273, -0.089395  , ..., -0.00798493,\n",
       "         -0.0449953 ,  0.0608698 ],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ]],\n",
       "\n",
       "       [[-0.0241599 ,  0.00913638,  0.0201729 , ...,  0.0293605 ,\n",
       "         -0.0651387 , -0.0222474 ],\n",
       "        [-0.0126048 ,  0.0212764 , -0.0241812 , ..., -0.00154248,\n",
       "          0.0122048 ,  0.0596878 ],\n",
       "        [-0.0460664 ,  0.00825271,  0.0143445 , ..., -0.0127337 ,\n",
       "          0.00497887,  0.0251865 ],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bcb3b0d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15131, 43, 300)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91e8816f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "(12104, 43, 300)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\Computer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Computer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Computer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Computer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\Computer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 918, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\Computer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\Computer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\Computer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\Computer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 1789, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"C:\\Users\\Computer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\backend.py\", line 5083, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None,) and (None, 43, 1) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining for fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_no\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(X_train[train]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 11\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m scores \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_train[test], Y_train[test], verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScore for fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_no\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mmetrics_names[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscores[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mmetrics_names[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscores[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1147\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m   1146\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1147\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[0;32m   1148\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\Computer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Computer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Computer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Computer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\Computer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 918, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\Computer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\Computer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\Computer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\Computer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 1789, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"C:\\Users\\Computer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\backend.py\", line 5083, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None,) and (None, 43, 1) are incompatible\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=5,shuffle=True)\n",
    "\n",
    "fold_no = 1;\n",
    "\n",
    "for train,test in kfold.split(X_train):\n",
    "    \n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "    \n",
    "    print(X_train[train].shape)\n",
    "    history = model.fit(X_train[train],Y_train[train])\n",
    "    scores = model.evaluate(X_train[test], Y_train[test], verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    loss_per_fold.append(scores[0])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
